{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing import Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User information retrieved successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'ef7c085b-fdb5-4c94-b7b6-a61a3d04c210',\n",
       " 'email': '2280905@gmail.com',\n",
       " 'is_allowed_send_init_message': True,\n",
       " 'username': 'Dmitriy Grankin',\n",
       " 'first_name': 'Dmitriy',\n",
       " 'last_name': 'Grankin',\n",
       " 'image': 'https://lh3.googleusercontent.com/a/ACg8ocLu4ZPfzj9KFNVqKL3NTF4SXUYvJ4mwoXLuoQ0BZzjw7OBIRA=s96-c',\n",
       " 'created_timestamp': '2024-05-28T11:16:50.894711Z',\n",
       " 'updated_timestamp': '2024-10-18T08:47:02.519781Z'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vexa import VexaAPI\n",
    "vexa = VexaAPI(token='3ae04e20124d40babc5107e658c666b6')\n",
    "await vexa.get_user_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing import check_item_exists, process_meeting_data, save_meeting_data_to_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "self = Indexing(token=vexa.token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_meetings = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_models import MeetingExtraction, EntityExtraction, SummaryIndexesRefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_meeting_data(formatted_input, df):\n",
    "    extraction_tasks = [\n",
    "        MeetingExtraction.extract(formatted_input),\n",
    "        EntityExtraction.extract(formatted_input)\n",
    "    ]\n",
    "    discussion_points_df, topics_df = await asyncio.gather(*extraction_tasks)\n",
    "    \n",
    "    discussion_points_df['model'] = 'MeetingExtraction'\n",
    "    topics_df['model'] = 'EntityExtraction'\n",
    "    \n",
    "    # Rename columns to match the new schema\n",
    "    discussion_points_df = discussion_points_df.rename(columns={'item': 'topic_name', 'type': 'topic_type'})\n",
    "    topics_df = topics_df.rename(columns={'entity': 'topic_name', 'type': 'topic_type'})\n",
    "    \n",
    "    # Combine the dataframes\n",
    "    summary_df = pd.concat([discussion_points_df, topics_df]).reset_index(drop=True)\n",
    "    \n",
    "    summary_refs = await SummaryIndexesRefs.extract(summary_df, formatted_input)\n",
    "\n",
    "    # Create a new dataframe for the references\n",
    "    ref_df = pd.DataFrame([(ref['summary_index'], r['s'], r['e']) \n",
    "                           for ref in summary_refs \n",
    "                           for r in ref['references']],\n",
    "                          columns=['summary_index', 'start', 'end'])\n",
    "\n",
    "    # Merge the ref_df with summary_df\n",
    "    entities_with_refs = summary_df.reset_index().rename(columns={'index': 'summary_index'})\n",
    "    entities_with_refs = entities_with_refs.merge(ref_df, on='summary_index', how='left')\n",
    "\n",
    "    # Function to extract text from df based on start and end indices, including speaker\n",
    "    def get_text_range_with_speaker(row):\n",
    "        text_range = df.loc[row['start']:row['end']]\n",
    "        return ' | '.join(f\"{speaker}: {content}\" for speaker, content in zip(text_range['speaker'], text_range['content']))\n",
    "\n",
    "    # Apply the function to get the referenced text with speakers\n",
    "    entities_with_refs['referenced_text'] = entities_with_refs.apply(get_text_range_with_speaker, axis=1)\n",
    "\n",
    "    # Group by summary_index to combine multiple references\n",
    " #   try:\n",
    "    final_df = entities_with_refs.groupby('summary_index').agg({\n",
    "        'topic_name': 'first',\n",
    "        'topic_type': 'first',\n",
    "        'summary': 'first',\n",
    "        'details': 'first',\n",
    "        'speaker': 'first',\n",
    "        'referenced_text': ' | '.join,\n",
    "        'model': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    return final_df\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error processing meeting data: {e}\")\n",
    "    #     return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request URL: http://127.0.0.1:8001/api/v1/calls/all\n",
      "Request Params: {'token': '3ae04e20124d40babc5107e658c666b6'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['details', 'speaker', 'summary', 'topic_name', 'topic_type'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transcription:   \n\u001b[1;32m     10\u001b[0m     df, formatted_input, start_datetime, speakers, transcript \u001b[38;5;241m=\u001b[39m transcription\n\u001b[0;32m---> 11\u001b[0m     final_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[1;32m     12\u001b[0m         process_meeting_data(formatted_input, df),\n\u001b[1;32m     13\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m save_meeting_data_to_db(final_df, meeting_id, transcript, start_datetime)\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/asyncio/tasks.py:479\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     fut\u001b[38;5;241m.\u001b[39mremove_done_callback(cb)\n",
      "Cell \u001b[0;32mIn[19], line 40\u001b[0m, in \u001b[0;36mprocess_meeting_data\u001b[0;34m(formatted_input, df)\u001b[0m\n\u001b[1;32m     36\u001b[0m    entities_with_refs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferenced_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m entities_with_refs\u001b[38;5;241m.\u001b[39mapply(get_text_range_with_speaker, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m    \u001b[38;5;66;03m# Group by summary_index to combine multiple references\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#   try:\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m    final_df \u001b[38;5;241m=\u001b[39m \u001b[43mentities_with_refs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopic_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopic_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetails\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspeaker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreferenced_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m | \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     48\u001b[0m \u001b[43m   \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     50\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/pandas/core/apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/pandas/core/apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/pandas/core/apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1607\u001b[0m ):\n\u001b[0;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/pandas/core/apply.py:462\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m is_groupby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[1;32m    461\u001b[0m func \u001b[38;5;241m=\u001b[39m cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[0;32m--> 462\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m is_non_unique_col \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    465\u001b[0m     selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(selected_obj\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/pandas/core/apply.py:663\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m     cols \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;241m.\u001b[39mdifference(obj\u001b[38;5;241m.\u001b[39mcolumns, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    665\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['details', 'speaker', 'summary', 'topic_name', 'topic_type'] do not exist\""
     ]
    }
   ],
   "source": [
    "meetings = await self.vexa.get_meetings()\n",
    "meetings = meetings[-num_meetings:]  # Process last N meetings\n",
    "\n",
    "for meeting in meetings:\n",
    "    meeting_id = meeting['id']\n",
    "    # try:\n",
    "    if not await check_item_exists(meeting_id):\n",
    "        transcription = await self.vexa.get_transcription(meeting_session_id=meeting_id, use_index=True)\n",
    "        if transcription:   \n",
    "            df, formatted_input, start_datetime, speakers, transcript = transcription\n",
    "            final_df = await asyncio.wait_for(\n",
    "                process_meeting_data(formatted_input, df),\n",
    "                timeout=60\n",
    "            )\n",
    "            await save_meeting_data_to_db(final_df, meeting_id, transcript, start_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "await indexing.index_meetings(user_id='', num_meetings=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
