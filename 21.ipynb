{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from vexa import VexaAPI\n",
    "from qdrant_search import QdrantSearchEngine\n",
    "\n",
    "from core import system_msg, user_msg, assistant_msg, generic_call_stream, count_tokens, BaseCall\n",
    "from prompts import Prompts\n",
    "from pydantic_models import ThreadName\n",
    "from thread_manager import ThreadManager\n",
    "from core import generic_call_\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SearchResult(BaseModel):\n",
    "    output: str\n",
    "    messages: List[dict]\n",
    "    meeting_ids: List[str]\n",
    "    full_context: str\n",
    "    thread_id: str\n",
    "    thread_name: str\n",
    "    indexed_meetings: dict\n",
    "    linked_output: str\n",
    "\n",
    "\n",
    "class SearchAssistant:\n",
    "    def __init__(self):\n",
    "        self.search_engine = QdrantSearchEngine()\n",
    "        self.thread_manager = None  # Initialize to None\n",
    "        self.prompts = Prompts()\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "        self.indexing_jobs = {}\n",
    "        \n",
    "    async def initialize(self):\n",
    "       self.thread_manager = await ThreadManager.create()  # Use the async create method\n",
    "       \n",
    "    \n",
    "    async def get_thread(self, thread_id: str):\n",
    "        return await self.thread_manager.get_thread(thread_id)\n",
    "\n",
    "    async def get_user_threads(self, user_id: str):\n",
    "        return await self.thread_manager.get_user_threads(user_id)\n",
    "\n",
    "    async def count_documents(self, user_id: str):\n",
    "        return await self.analyzer.count_documents(user_id=user_id)\n",
    "\n",
    "    async def get_messages_by_thread_id(self, thread_id: str):\n",
    "        return await self.thread_manager.get_messages_by_thread_id(thread_id)\n",
    "\n",
    "    async def delete_thread(self, thread_id: str) -> bool:\n",
    "        return await self.thread_manager.delete_thread(thread_id)\n",
    "\n",
    "    async def is_indexing(self, user_id: str) -> bool:\n",
    "        return self.indexing_jobs.get(user_id, False)\n",
    "\n",
    "    async def remove_user_data(self, user_id: str) -> int:\n",
    "        return await self.analyzer.remove_user_data(user_id)\n",
    "\n",
    "    # The following methods should be updated to be async if they involve I/O operations\n",
    "    async def parse_refs(self, text):\n",
    "        pattern = r'\\[(\\d+)\\]'\n",
    "        return list(set(re.findall(pattern, text)))\n",
    "\n",
    "    async def get_indexed_meetings(self, meeting_ids, refs):\n",
    "        indexed_meetings = {}\n",
    "        for i, meeting_id in enumerate(meeting_ids):\n",
    "            if str(i + 1) in refs:\n",
    "                indexed_meetings[str(i + 1)] = meeting_id\n",
    "        return indexed_meetings\n",
    "\n",
    "    async def embed_links(self, text, url_dict):\n",
    "        for key, url in url_dict.items():\n",
    "            text = text.replace(f'[{key}]', f'[{key}]({url})')\n",
    "        return text\n",
    "    \n",
    "    async def search(self, query: str) -> tuple[str, list]:\n",
    "        # Get search results\n",
    "        main_results = await self.search_engine.search(\n",
    "            query_text=query,\n",
    "            limit=200,\n",
    "            min_score=0.4,\n",
    "        )\n",
    "\n",
    "        speaker_results = await self.search_engine.search_by_speaker(\n",
    "            speaker_query=query,\n",
    "            limit=200,\n",
    "            min_score=0.49\n",
    "        )\n",
    "\n",
    "        # Process results into DataFrames\n",
    "        main_df = pd.DataFrame(main_results) if main_results else pd.DataFrame()\n",
    "        speaker_df = pd.DataFrame(speaker_results) if speaker_results else pd.DataFrame()\n",
    "\n",
    "        # Select relevant columns and combine results\n",
    "        columns = ['topic_name', 'speaker_name', 'summary', 'details','meeting_id','timestamp']\n",
    "        score_columns = ['score', 'vector_scores', 'exact_matches']\n",
    "\n",
    "        if len(main_df) > 0:\n",
    "            main_df = main_df[columns + score_columns]\n",
    "            main_df['source'] = 'main'\n",
    "        else:\n",
    "            main_df = pd.DataFrame(columns=columns + score_columns + ['source'])\n",
    "\n",
    "        if len(speaker_df) > 0:\n",
    "            speaker_df = speaker_df[columns + ['score']]  # Speaker search has simpler scoring\n",
    "            speaker_df['source'] = 'speaker'\n",
    "        else:\n",
    "            speaker_df = pd.DataFrame(columns=columns + ['score', 'source'])\n",
    "\n",
    "        # Combine, deduplicate and sort results\n",
    "        self.results = pd.concat([main_df, speaker_df]).drop_duplicates(subset=columns).reset_index(drop=True)\n",
    "        if not self.results.empty:\n",
    "            self.results = self.results.sort_values('score', ascending=False)\n",
    "            \n",
    "        return self.results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_context(search_results):\n",
    "    columns = ['topic_name', 'speaker_name', 'summary', 'details','meeting_id']\n",
    "    context = search_results[columns].to_markdown(index=False) if not search_results.empty else \"No relevant context found.\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def normalize_series(series: pd.Series) -> pd.Series:\n",
    "\n",
    "    min_value = series.min()\n",
    "    max_value = series.max()\n",
    "    normalized = (series - min_value) / (max_value - min_value)\n",
    "    return normalized * 0.5 + 0.5  # Scale to range [0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = Prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = SearchAssistant()\n",
    "await self.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'vexa'\n",
    "thread_id = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = await self.search('vexa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if thread_id:\n",
    "    thread = await self.thread_manager.get_thread(thread_id)\n",
    "    if not thread:\n",
    "        raise ValueError(f\"Thread with id {thread_id} not found\")\n",
    "    messages = thread.messages\n",
    "    thread_name = thread.thread_name\n",
    "else:\n",
    "    messages = []\n",
    "    thread_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_context(search_results: pd.DataFrame) -> str:\n",
    "    search_results['relevance_score'] = normalize_series(search_results['score']).round(2)\n",
    "    search_results = search_results.sort_values('timestamp').reset_index(drop=True)\n",
    "    search_results['datetime'] = pd.to_datetime(search_results['timestamp']).dt.strftime('%A %Y-%m-%d %H:%M')\n",
    "\n",
    "    meetinds_df = search_results[['meeting_id']].drop_duplicates().reset_index(drop=True)\n",
    "    meetinds_df['meeting_index'] = meetinds_df.index + 1\n",
    "    prepared_df = search_results.drop(columns=['timestamp', 'vector_scores', 'exact_matches', 'source', 'score']).merge(meetinds_df, on='meeting_id').drop(columns=['meeting_id'])\n",
    "    \n",
    "    meetings = meetinds_df.to_dict(orient='records')\n",
    "    \n",
    "    return prepared_df.to_markdown(index=False) if not prepared_df.empty else \"No relevant context found.\",  {meeting['meeting_index']: meeting['meeting_id'] for meeting in meetings}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "context, indexed_meetings = prep_context(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = {k: f'https://dashboard.vexa.ai/#{v}' for k, v in indexed_meetings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context_msg = system_msg(f\"Context: {context}\")\n",
    "\n",
    "# Generate response\n",
    "messages = [\n",
    "    system_msg(self.prompts.perplexity),\n",
    "    *messages,\n",
    "    context_msg\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vexa is an AI meeting assistant designed to enhance the efficiency of meetings by providing real-time transcription and contextual support. It captures spoken dialogue during meetings, allowing participants to follow along with a live transcript. Vexa also features an Assistant tab that offers summaries and additional assistance, making it a comprehensive tool for managing meeting discussions[1][5][10].\n",
      "\n",
      "The product is currently in the testing phase, with plans for a paid subscription model to generate revenue. User feedback is being actively sought to refine its features and improve user experience. Vexa aims to assist users in capturing and understanding information in real-time, particularly in diverse professional settings[1][9][54].\n"
     ]
    }
   ],
   "source": [
    "response = await generic_call_stream(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\"\n",
    "async for chunk in generic_call_(messages,streaming=True):\n",
    "    output += chunk\n",
    "   # yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def embed_links(text, url_dict):\n",
    "    # First, add a space between consecutive reference numbers\n",
    "    text = re.sub(r'\\]\\[', '] [', text)\n",
    "    \n",
    "    # Then replace each reference with its link\n",
    "    for key, url in url_dict.items():\n",
    "        text = text.replace(f' [{key}]', f'[{key}]({url})')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_output = await embed_links(output, url_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vexa is an AI meeting assistant designed to facilitate meetings by providing real-time transcription and assistance. It captures spoken dialogue during meetings, allowing participants to follow along with a live written record. Vexa also features an Assistant tab that provides access to summaries, questions, and additional support during discussions[1](https://dashboard.vexa.ai/#c5828919-ac2d-4fe9-bd7a-8eaa91f743c8) [5](https://dashboard.vexa.ai/#ba7c1f92-a033-4302-bc60-a57ef802d232) [10](https://dashboard.vexa.ai/#1b8005e2-bf6f-461f-813e-2955c2e8d46a). \n",
      "\n",
      "The product is currently in the testing phase, focusing on enhancing user experience and functionality based on feedback. It aims to improve corporate communication by structuring and returning relevant information in real-time, making it particularly useful for users interacting with diverse accents and industries[1](https://dashboard.vexa.ai/#c5828919-ac2d-4fe9-bd7a-8eaa91f743c8) [9](https://dashboard.vexa.ai/#e1d2fbb4-8f91-42bd-99d2-438d13778da9) [24](https://dashboard.vexa.ai/#f0d5f231-b866-4cb4-b59d-73899f2e0dc9). \n",
      "\n",
      "Vexa operates as a Chrome extension, which minimizes its presence during calls, and it is being marketed through various strategies, including influencer partnerships and social media campaigns[1](https://dashboard.vexa.ai/#c5828919-ac2d-4fe9-bd7a-8eaa91f743c8) [54](https://dashboard.vexa.ai/#70cd7290-801a-4caf-9786-359cc6e16c60).\n"
     ]
    }
   ],
   "source": [
    "print(linked_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Vexa is an AI meeting assistant designed to facilitate meetings by providing real-time transcription and assistance. It captures spoken dialogue during meetings, allowing participants to follow along with a live written record. Vexa also features an Assistant tab that provides access to summaries, questions, and additional support during discussions[1](https://dashboard.vexa.ai/#c5828919-ac2d-4fe9-bd7a-8eaa91f743c8) [5](https://dashboard.vexa.ai/#ba7c1f92-a033-4302-bc60-a57ef802d232) [10](https://dashboard.vexa.ai/#1b8005e2-bf6f-461f-813e-2955c2e8d46a). \n",
       "\n",
       "The product is currently in the testing phase, focusing on enhancing user experience and functionality based on feedback. It aims to improve corporate communication by structuring and returning relevant information in real-time, making it particularly useful for users interacting with diverse accents and industries[1](https://dashboard.vexa.ai/#c5828919-ac2d-4fe9-bd7a-8eaa91f743c8) [9](https://dashboard.vexa.ai/#e1d2fbb4-8f91-42bd-99d2-438d13778da9) [24](https://dashboard.vexa.ai/#f0d5f231-b866-4cb4-b59d-73899f2e0dc9). \n",
       "\n",
       "Vexa operates as a Chrome extension, which minimizes its presence during calls, and it is being marketed through various strategies, including influencer partnerships and social media campaigns[1](https://dashboard.vexa.ai/#c5828919-ac2d-4fe9-bd7a-8eaa91f743c8) [54](https://dashboard.vexa.ai/#70cd7290-801a-4caf-9786-359cc6e16c60)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(linked_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(user_msg(query))\n",
    "messages.append(assistant_msg(msg=linked_output, service_content=output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parse_refs(text):\n",
    "    pattern = r'\\[(\\d+)\\]'\n",
    "    refs = list(set(re.findall(pattern, text)))\n",
    "    \n",
    "    return [int(r) for r in refs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 54, 5, 12]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await parse_refs(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def chat(self, user_id: str, query: str, user_name: str='', thread_id: Optional[str] = None, model: Optional[str] = None, temperature: Optional[float] = None, debug: bool = False):\n",
    "    if thread_id:\n",
    "        thread = await self.thread_manager.get_thread(thread_id)\n",
    "        if not thread:\n",
    "            raise ValueError(f\"Thread with id {thread_id} not found\")\n",
    "        messages = thread.messages\n",
    "        thread_name = thread.thread_name\n",
    "    else:\n",
    "        messages = []\n",
    "        thread_name = None\n",
    "\n",
    "    query_ = ' '.join([m.content for m in messages]) + ' ' + query\n",
    "    queries = await self.analyzer.generate_search_queries(query_, user_id=user_id, user_name=user_name)\n",
    "    \n",
    "    summaries = await self.analyzer.get_summaries(user_id=user_id, user_name=user_name)\n",
    "    full_context, meeting_ids = await self.analyzer.build_context(queries, summaries, include_all_summaries=False, user_id=user_id, user_name=user_name, k=20)\n",
    "\n",
    "    pref = \"Based on the following context, answer the question:\" if len(messages) == 0 else \"Follow-up request:\"\n",
    "    user_info = f\"The User is {user_name}\"\n",
    "    messages_context = [\n",
    "        system_msg(self.prompts.perplexity + f'. {user_info}'), \n",
    "        user_msg(f\"Context:\\n{full_context}\"),\n",
    "    ] + messages + [user_msg(f\"{pref} {query}. Always supply references to meetings as [1][2][3] etc.\")]\n",
    "\n",
    "    model_to_use = model or self.model\n",
    "\n",
    "    output = \"\"\n",
    "    async for chunk in generic_call_(messages_context, model=model_to_use, temperature=temperature, streaming=True):\n",
    "        output += chunk\n",
    "        yield chunk\n",
    "    \n",
    "    indexed_meetings = await self.get_indexed_meetings(meeting_ids, await self.parse_refs(output))\n",
    "    url_dict = {k: f'https://dashboard.vexa.ai/#{v}' for k, v in indexed_meetings.items()}\n",
    "    linked_output = await self.embed_links(output, url_dict)\n",
    "    \n",
    "    messages.append(user_msg(query))\n",
    "    messages.append(assistant_msg(msg=linked_output, service_content=output))\n",
    "\n",
    "    if not thread_id:\n",
    "        messages_str = ';'.join([m.content for m in messages if m.role == 'user'])\n",
    "        thread_name = await ThreadName.call([user_msg(messages_str)])\n",
    "        thread_name = thread_name[0].thread_name\n",
    "        thread_id = await self.thread_manager.upsert_thread(user_id=user_id, thread_name=thread_name, messages=messages)\n",
    "    else:\n",
    "        await self.thread_manager.upsert_thread(user_id=user_id, messages=messages, thread_id=thread_id)\n",
    "\n",
    "    result = {\n",
    "        \"thread_id\": thread_id,\n",
    "        \"linked_output\": linked_output\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        result.update({\n",
    "            \"output\": output,\n",
    "            \"summaries\": summaries,\n",
    "            \"full_context\": full_context,\n",
    "            \"meeting_ids\": meeting_ids,\n",
    "            \"queries\": queries,\n",
    "        })\n",
    "\n",
    "    yield result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
