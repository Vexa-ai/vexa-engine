{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from vexa import VexaAPI\n",
    "from qdrant_search import QdrantSearchEngine\n",
    "\n",
    "from core import system_msg, user_msg, assistant_msg, generic_call_stream, count_tokens, BaseCall\n",
    "from prompts import Prompts\n",
    "from pydantic_models import ThreadName\n",
    "from thread_manager import ThreadManager\n",
    "from core import generic_call_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SearchResult(BaseModel):\n",
    "    output: str\n",
    "    messages: List[dict]\n",
    "    meeting_ids: List[str]\n",
    "    full_context: str\n",
    "    thread_id: str\n",
    "    thread_name: str\n",
    "    indexed_meetings: dict\n",
    "    linked_output: str\n",
    "\n",
    "\n",
    "class SearchAssistant:\n",
    "    def __init__(self):\n",
    "        self.search_engine = QdrantSearchEngine()\n",
    "        self.thread_manager = None  # Initialize to None\n",
    "        self.prompts = Prompts()\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "        self.indexing_jobs = {}\n",
    "        \n",
    "    async def initialize(self):\n",
    "       self.thread_manager = await ThreadManager.create()  # Use the async create method\n",
    "       \n",
    "    \n",
    "    async def get_thread(self, thread_id: str):\n",
    "        return await self.thread_manager.get_thread(thread_id)\n",
    "\n",
    "    async def get_user_threads(self, user_id: str):\n",
    "        return await self.thread_manager.get_user_threads(user_id)\n",
    "\n",
    "    async def count_documents(self, user_id: str):\n",
    "        return await self.analyzer.count_documents(user_id=user_id)\n",
    "\n",
    "    async def get_messages_by_thread_id(self, thread_id: str):\n",
    "        return await self.thread_manager.get_messages_by_thread_id(thread_id)\n",
    "\n",
    "    async def delete_thread(self, thread_id: str) -> bool:\n",
    "        return await self.thread_manager.delete_thread(thread_id)\n",
    "\n",
    "    async def is_indexing(self, user_id: str) -> bool:\n",
    "        return self.indexing_jobs.get(user_id, False)\n",
    "\n",
    "    async def remove_user_data(self, user_id: str) -> int:\n",
    "        return await self.analyzer.remove_user_data(user_id)\n",
    "\n",
    "    # The following methods should be updated to be async if they involve I/O operations\n",
    "    async def parse_refs(self, text):\n",
    "        pattern = r'\\[(\\d+)\\]'\n",
    "        return list(set(re.findall(pattern, text)))\n",
    "\n",
    "    async def get_indexed_meetings(self, meeting_ids, refs):\n",
    "        indexed_meetings = {}\n",
    "        for i, meeting_id in enumerate(meeting_ids):\n",
    "            if str(i + 1) in refs:\n",
    "                indexed_meetings[str(i + 1)] = meeting_id\n",
    "        return indexed_meetings\n",
    "\n",
    "    async def embed_links(self, text, url_dict):\n",
    "        for key, url in url_dict.items():\n",
    "            text = text.replace(f'[{key}]', f'[{key}]({url})')\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = SearchAssistant()\n",
    "await self.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "thread_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'vexa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results = await self.search_engine.search(\n",
    "    query_text=query,\n",
    "    limit=200,\n",
    "    min_score=0.4,\n",
    ")\n",
    "\n",
    "speaker_results = await self.search_engine.search_by_speaker(\n",
    "    speaker_query=query,\n",
    "    limit=200,\n",
    "    min_score=0.49\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def chat(self, user_id: str, query: str, user_name: str='', thread_id: Optional[str] = None, model: Optional[str] = None, temperature: Optional[float] = None, debug: bool = False):\n",
    "    if thread_id:\n",
    "        thread = await self.thread_manager.get_thread(thread_id)\n",
    "        if not thread:\n",
    "            raise ValueError(f\"Thread with id {thread_id} not found\")\n",
    "        messages = thread.messages\n",
    "        thread_name = thread.thread_name\n",
    "    else:\n",
    "        messages = []\n",
    "        thread_name = None\n",
    "\n",
    "    query_ = ' '.join([m.content for m in messages]) + ' ' + query\n",
    "    queries = await self.analyzer.generate_search_queries(query_, user_id=user_id, user_name=user_name)\n",
    "    \n",
    "    summaries = await self.analyzer.get_summaries(user_id=user_id, user_name=user_name)\n",
    "    full_context, meeting_ids = await self.analyzer.build_context(queries, summaries, include_all_summaries=False, user_id=user_id, user_name=user_name, k=20)\n",
    "\n",
    "    pref = \"Based on the following context, answer the question:\" if len(messages) == 0 else \"Follow-up request:\"\n",
    "    user_info = f\"The User is {user_name}\"\n",
    "    messages_context = [\n",
    "        system_msg(self.prompts.perplexity + f'. {user_info}'), \n",
    "        user_msg(f\"Context:\\n{full_context}\"),\n",
    "    ] + messages + [user_msg(f\"{pref} {query}. Always supply references to meetings as [1][2][3] etc.\")]\n",
    "\n",
    "    model_to_use = model or self.model\n",
    "\n",
    "    output = \"\"\n",
    "    async for chunk in generic_call_(messages_context, model=model_to_use, temperature=temperature, streaming=True):\n",
    "        output += chunk\n",
    "        yield chunk\n",
    "    \n",
    "    indexed_meetings = await self.get_indexed_meetings(meeting_ids, await self.parse_refs(output))\n",
    "    url_dict = {k: f'https://dashboard.vexa.ai/#{v}' for k, v in indexed_meetings.items()}\n",
    "    linked_output = await self.embed_links(output, url_dict)\n",
    "    \n",
    "    messages.append(user_msg(query))\n",
    "    messages.append(assistant_msg(msg=linked_output, service_content=output))\n",
    "\n",
    "    if not thread_id:\n",
    "        messages_str = ';'.join([m.content for m in messages if m.role == 'user'])\n",
    "        thread_name = await ThreadName.call([user_msg(messages_str)])\n",
    "        thread_name = thread_name[0].thread_name\n",
    "        thread_id = await self.thread_manager.upsert_thread(user_id=user_id, thread_name=thread_name, messages=messages)\n",
    "    else:\n",
    "        await self.thread_manager.upsert_thread(user_id=user_id, messages=messages, thread_id=thread_id)\n",
    "\n",
    "    result = {\n",
    "        \"thread_id\": thread_id,\n",
    "        \"linked_output\": linked_output\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        result.update({\n",
    "            \"output\": output,\n",
    "            \"summaries\": summaries,\n",
    "            \"full_context\": full_context,\n",
    "            \"meeting_ids\": meeting_ids,\n",
    "            \"queries\": queries,\n",
    "        })\n",
    "\n",
    "    yield result\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
