{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nest_asyncio import apply\n",
    "apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psql_helpers import (\n",
    "    async_session, get_session,\n",
    "\n",
    ")\n",
    "from vexa import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vexa token: 3ae04e20124d40babc5107e658c666b6\n",
      "User information retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "auth = VexaAuth()\n",
    "vexa = VexaAPI(token=\"\")\n",
    "user_id = (await vexa.get_user_info())['id']\n",
    "\n",
    "# Initialize search engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Elasticsearch at elasticsearch:9200\n"
     ]
    }
   ],
   "source": [
    "qdrant_engine = QdrantSearchEngine(os.getenv('VOYAGE_API_KEY'))\n",
    "es_engine = await ElasticsearchBM25.create() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_manager = UnifiedChatManager(\n",
    "        session=get_session(),\n",
    "        qdrant_engine=qdrant_engine,\n",
    "        es_engine=es_engine\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the weather in San Francisco?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fronten_id = None\n",
    "thread_id = None\n",
    "content_id = None\n",
    "entity_id = None\n",
    "content_ids = None\n",
    "entity_ids = None\n",
    "model = \"gpt-4o-mini\"\n",
    "temperature = 0.0\n",
    "meta = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import jupyter_stream_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psql_helpers import get_session\n",
    "\n",
    "@jupyter_stream_output(markdown=True)\n",
    "async def unified_chat(\n",
    "    query: str,\n",
    "    content_id: Optional[UUID] = None,\n",
    "    entity_id: Optional[int] = None,\n",
    "    content_ids: Optional[List[UUID]] = None,\n",
    "    entity_ids: Optional[List[int]] = None,\n",
    "    thread_id: Optional[str] = None,\n",
    "    temperature: float = 0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Unified chat function supporting both content and entity-based search\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query\n",
    "        content_id: Optional single content ID for direct thread mapping\n",
    "        entity_id: Optional single entity ID for direct thread mapping\n",
    "        content_ids: Optional list of content IDs for search scope\n",
    "        entity_ids: Optional list of entity IDs for search scope\n",
    "        thread_id: Optional thread ID to continue conversation\n",
    "        temperature: Model temperature parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    async with get_session() as session:\n",
    "        chat_manager = UnifiedChatManager(\n",
    "            session=session,\n",
    "            qdrant_engine=qdrant_engine,\n",
    "            es_engine=es_engine\n",
    "        )\n",
    "        \n",
    "        async for result in chat_manager.chat(\n",
    "            user_id=user_id,\n",
    "            query=query,\n",
    "            content_id=content_id,\n",
    "            entity_id=entity_id,\n",
    "            content_ids=content_ids,\n",
    "            entity_ids=entity_ids,\n",
    "            thread_id=thread_id,\n",
    "            temperature=temperature\n",
    "        ):\n",
    "            if 'chunk' in result:\n",
    "                yield result['chunk']\n",
    "            elif 'linked_output' in result:\n",
    "                yield result\n",
    "                \n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is **Dmitriy Grankin**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': '.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'thread_id': 'dcbae2f5-a34e-423b-85b8-b832cf3cea1c',\n",
       " 'output': 'Your name is **Dmitriy Grankin**.',\n",
       " 'linked_output': 'Your name is **Dmitriy Grankin**.',\n",
       " 'service_content': {'output': 'Your name is **Dmitriy Grankin**.',\n",
       "  'context': 'No relevant context found.'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await unified_chat(query=\"what is my name?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It seems there isn't enough context to identify \"this guy.\" If you could provide more details or specify who you are referring to, I would be happy to help!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': '!'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'thread_id': 'ef7f61aa-4602-448b-aeed-622b47a741d0',\n",
       " 'output': 'It seems there isn\\'t enough context to identify \"this guy.\" If you could provide more details or specify who you are referring to, I would be happy to help!',\n",
       " 'linked_output': 'It seems there isn\\'t enough context to identify \"this guy.\" If you could provide more details or specify who you are referring to, I would be happy to help!',\n",
       " 'service_content': {'output': 'It seems there isn\\'t enough context to identify \"this guy.\" If you could provide more details or specify who you are referring to, I would be happy to help!',\n",
       "  'context': 'No relevant context found.'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await unified_chat(query=\"who is this guy?\",entity_id=5432)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
